{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed01417",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi-input time series\n",
    "## include high, low, open, close, volumn\n",
    "\n",
    "#Parameters:\n",
    "NUM_OF_YEARS = 20\n",
    "STOCKS = ['GOOG', 'TSLA', 'XOM', 'UNH', 'JNJ', 'JPM', 'V', 'PG', 'HD', 'CVX', 'MA', 'LLY', 'MRK', 'MSFT' ]\n",
    "NUM_LSTM_DAYS = 60\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91844840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, LSTM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7ad8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_close(df):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('Close Price History')\n",
    "    plt.plot(df['Close'])\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel('Close Price USD', fontsize=18)\n",
    "    plt.show()\n",
    "\n",
    "def get_data(stock):\n",
    "    end = datetime.today()\n",
    "    start = datetime(end.year-NUM_OF_YEARS,end.month,end.day)\n",
    "    data_path = 'data/' + stock +\"_\" + str(start.year)+str(end.year)+\".csv\"\n",
    "    if not os.path.exists('data/'):\n",
    "        os.makedirs('data/')\n",
    "    df = pd.DataFrame()\n",
    "    my_file = Path(data_path)\n",
    "    if not my_file.is_file():\n",
    "        print(\"loading_data\")\n",
    "        df = yf.download(stock, start=start, end=end)\n",
    "        df.to_csv(data_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.set_index('Date')\n",
    "    return df\n",
    "\n",
    "\n",
    "def init_model(x_train_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, return_sequences=True, input_shape= (x_train_shape[1], x_train_shape[2]), dropout=0.1 ))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23109e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(stock, model):\n",
    "    df = get_data(stock)\n",
    "\n",
    "    high_dataset = df[['High']].values\n",
    "    low_dataset = df[['Low']].values\n",
    "    open_dataset = df[['Open']].values\n",
    "    volumn_dataset = df[['Volume']].values\n",
    "    close_dataset = df[['Close']].values\n",
    "\n",
    "    training_data_len = math.ceil(len(close_dataset)*0.8)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    volumn_dataset = scaler.fit_transform(volumn_dataset)\n",
    "\n",
    "    high_train_data = high_dataset[0:training_data_len, :]\n",
    "    low_train_data = low_dataset[0:training_data_len, :]\n",
    "    open_train_data = open_dataset[0:training_data_len, :]\n",
    "    volumn_train_data = volumn_dataset[0:training_data_len, :]\n",
    "    close_train_data = close_dataset[0:training_data_len, :]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1000))\n",
    "    scaler.fit(high_train_data+high_train_data)\n",
    "\n",
    "    high_train_data = scaler.transform(high_train_data)\n",
    "    low_train_data = scaler.transform(low_train_data)\n",
    "    open_train_data = scaler.transform(open_train_data)\n",
    "    close_train_data = scaler.transform(close_train_data)\n",
    "\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(NUM_LSTM_DAYS, len(high_train_data)):\n",
    "        temp = []\n",
    "        for j in range(i-NUM_LSTM_DAYS, i):\n",
    "            temp.append([high_train_data[j,0], low_train_data[j,0], open_train_data[j,0], \n",
    "                        close_train_data[j,0], volumn_train_data[j,0] ])\n",
    "        x_train.append(temp)\n",
    "        y_train.append(close_train_data[i,0])\n",
    "\n",
    "    np.array(x_train).shape\n",
    "\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "    x_train.shape\n",
    "\n",
    "    if(model == None):\n",
    "        model = init_model(x_train.shape)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        model.fit(x_train, y_train, batch_size=64, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "    if not os.path.exists('model/'):\n",
    "        os.makedirs('model/')\n",
    "    model.save('model/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f611f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(stock, model):\n",
    "    df = get_data(stock)\n",
    "    high_dataset = df[['High']].values\n",
    "    low_dataset = df[['Low']].values\n",
    "    open_dataset = df[['Open']].values\n",
    "    volumn_dataset = df[['Volume']].values\n",
    "    close_dataset = df[['Close']].values\n",
    "\n",
    "    training_data_len = math.ceil(len(close_dataset)*0.8)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    volumn_dataset = scaler.fit_transform(volumn_dataset)\n",
    "\n",
    "    hight_test_data = high_dataset[training_data_len - NUM_LSTM_DAYS:, :]\n",
    "    low_test_data = low_dataset[training_data_len - NUM_LSTM_DAYS:, :]\n",
    "    open_test_data = open_dataset[training_data_len - NUM_LSTM_DAYS:, :]\n",
    "    volumn_test_data = volumn_dataset[training_data_len - NUM_LSTM_DAYS:, :]\n",
    "    close_test_data = close_dataset[training_data_len - NUM_LSTM_DAYS:, :]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(hight_test_data+low_test_data)\n",
    "\n",
    "    hight_test_data = scaler.transform(hight_test_data)\n",
    "    low_test_data = scaler.transform(low_test_data)\n",
    "    open_test_data = scaler.transform(open_test_data)\n",
    "    close_test_data = scaler.transform(close_test_data)\n",
    "\n",
    "    x_test = []\n",
    "    y_test = close_dataset[training_data_len:, :]\n",
    "\n",
    "    for i in range(NUM_LSTM_DAYS, len(hight_test_data)):\n",
    "        temp = []\n",
    "        for j in range(i-NUM_LSTM_DAYS, i):\n",
    "            temp.append([hight_test_data[j,0], low_test_data[j,0], open_test_data[j,0], \n",
    "                        close_test_data[j,0], volumn_test_data[j,0] ])\n",
    "        x_test.append(temp)\n",
    "        \n",
    "    x_test = np.array(x_test)\n",
    "    x_test.shape\n",
    "\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    predictions.size\n",
    "    rmse = np.sqrt(np.mean(predictions-y_test)**2)\n",
    "    print(\"rmse: \", rmse)\n",
    "\n",
    "    train = df.filter(['Close'])[:training_data_len]\n",
    "    valid = df.filter(['Close'])[training_data_len:]\n",
    "    print(\"valid: \", valid)\n",
    "\n",
    "    valid['Predictions'] = predictions\n",
    "    prev = valid['Close'][0]\n",
    "    sum = 0\n",
    "    count = 0 \n",
    "    origin_volatility = []\n",
    "    new_volatility = []\n",
    "    incorrect_date = []\n",
    "    incorrect_date.append(0)\n",
    "    for idx, p in enumerate(valid['Close'][1:]):\n",
    "        sum+=abs(p-prev)\n",
    "        if( (p-prev>0 and valid['Predictions'][idx]-prev<0) or (p-prev<0 and valid['Predictions'][idx]-prev>0)):\n",
    "            count +=1\n",
    "            incorrect_date.append(10)\n",
    "        else:\n",
    "            incorrect_date.append(-10)\n",
    "        origin_volatility.append((p-prev)*1)\n",
    "        new_volatility.append((valid['Predictions'][idx]-prev)*1)\n",
    "        prev=p\n",
    "        \n",
    "    print(\"Avg volatility: \", sum/(valid['Close'].size-1))\n",
    "    print(\"Wrong direction count: \", count, \"/\", (valid['Close'].size-1))\n",
    "\n",
    "    origin_volatility.insert(0,0)\n",
    "    new_volatility.insert(0,0)\n",
    "    valid['origin_volatility'] = origin_volatility\n",
    "    valid['new_volatility'] = new_volatility\n",
    "    valid['incorrect_date'] = incorrect_date\n",
    "\n",
    "    plt.figure(figsize=(32,8))\n",
    "    plt.title('Model')\n",
    "    plt.xlabel('Date', fontsize=18)\n",
    "    plt.ylabel('Close Price USD', fontsize=18)\n",
    "    #plt.plot(train['Close'])\n",
    "    plt.plot(valid[['origin_volatility','new_volatility','Close', 'Predictions','incorrect_date']])\n",
    "    plt.legend(['origin_volatility','new_volatility', 'Close', 'Predications','incorrect_date'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a89455b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Train stock:  GOOG  ----------------------\n",
      "----------------------------------------------------------\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "for stock in STOCKS:\n",
    "    print(\"-----------Train stock: \", stock, \" ----------------------\")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    train_data(stock, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf21734",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: Custom>Adam. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mks\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_data(\u001b[39m'\u001b[39m\u001b[39mAAPL\u001b[39m\u001b[39m'\u001b[39m, ks\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmodel/\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m       filepath \u001b[39m=\u001b[39m path_to_string(filepath)\n\u001b[0;32m    205\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 206\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39;49mload(filepath, \u001b[39mcompile\u001b[39;49m, options)\n\u001b[0;32m    208\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    209\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mUnable to load model. Filepath is not an hdf5 file (or h5py is not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mavailable) or SavedModel.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py:168\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    165\u001b[0m training_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_serialized_attributes[\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(\n\u001b[0;32m    166\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtraining_config\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m training_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m   model\u001b[39m.\u001b[39mcompile(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msaving_utils\u001b[39m.\u001b[39;49mcompile_args_from_training_config(\n\u001b[0;32m    169\u001b[0m       training_config), from_serialized\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m   saving_utils\u001b[39m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[0;32m    171\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model\u001b[39m.\u001b[39moptimizer, optimizer_v2\u001b[39m.\u001b[39mOptimizerV2):\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saving_utils.py:207\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mwith\u001b[39;00m generic_utils\u001b[39m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[0;32m    206\u001b[0m   optimizer_config \u001b[39m=\u001b[39m training_config[\u001b[39m'\u001b[39m\u001b[39moptimizer_config\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 207\u001b[0m   optimizer \u001b[39m=\u001b[39m optimizers\u001b[39m.\u001b[39;49mdeserialize(optimizer_config)\n\u001b[0;32m    209\u001b[0m   \u001b[39m# Recover losses.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\optimizers.py:94\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m all_classes:\n\u001b[0;32m     93\u001b[0m   config[\u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlower()\n\u001b[1;32m---> 94\u001b[0m \u001b[39mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m     95\u001b[0m     config,\n\u001b[0;32m     96\u001b[0m     module_objects\u001b[39m=\u001b[39;49mall_classes,\n\u001b[0;32m     97\u001b[0m     custom_objects\u001b[39m=\u001b[39;49mcustom_objects,\n\u001b[0;32m     98\u001b[0m     printable_module_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moptimizer\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:659\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(identifier, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    657\u001b[0m   \u001b[39m# In this case we are dealing with a Keras config dictionary.\u001b[39;00m\n\u001b[0;32m    658\u001b[0m   config \u001b[39m=\u001b[39m identifier\n\u001b[1;32m--> 659\u001b[0m   (\u001b[39mcls\u001b[39m, cls_config) \u001b[39m=\u001b[39m class_and_config_for_serialized_keras_object(\n\u001b[0;32m    660\u001b[0m       config, module_objects, custom_objects, printable_module_name)\n\u001b[0;32m    662\u001b[0m   \u001b[39m# If this object has already been loaded (i.e. it's shared between multiple\u001b[39;00m\n\u001b[0;32m    663\u001b[0m   \u001b[39m# objects), return the already-loaded object.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m   shared_object_id \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(SHARED_OBJECT_KEY)\n",
      "File \u001b[1;32mc:\\Users\\Gary\\anaconda3\\envs\\lstm2\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:556\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m get_registered_object(class_name, custom_objects, module_objects)\n\u001b[0;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 556\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. Please ensure this object is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    558\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mpassed to the `custom_objects` argument. See \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    559\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    560\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    561\u001b[0m       \u001b[39m.\u001b[39mformat(printable_module_name, class_name))\n\u001b[0;32m    563\u001b[0m cls_config \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    564\u001b[0m \u001b[39m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[39m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown optimizer: Custom>Adam. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "import tensorflow.python.keras as ks\n",
    "test_data('AAPL', ks.models.load_model('model/'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b61ee595f1c32953b6ddba615420cf019e07d858b0db6b22be825cf8c5e56d5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
